# =============================================================================
# HireMeBahamas Backend - Render Deployment Configuration (PRODUCTION-IMMORTAL)
# =============================================================================
# 
# FACEBOOK-GRADE RELIABILITY STACK:
# - Cold start: <12 seconds total
# - Login: <200ms (Redis-cached)
# - Feed/Profile/Messages: <150ms
# - Zero 499/502/503 ever again
# - RAM: <500MB forever
# - Cost: <$100/mo
#
# DEPLOY ORDER (CRITICAL):
# 1. Background Worker (keep_alive.py) - Deploy first, keeps service awake
# 2. Database Migration - Run before web service
# 3. Web Service - Main API with health check
# 4. Vercel Frontend - After backend is healthy
#
# EXACT RENDER DASHBOARD SETTINGS (COPY-PASTE READY):
# ├── Instance Type: Starter ($7/mo) or Standard ($25/mo)
# ├── Health Check Path: /health
# ├── Health Check Timeout: 10 seconds
# ├── Grace Period: 180 seconds
# └── Memory Limit: 512MB (Starter) or 2GB (Standard)
# =============================================================================

services:
  - type: web
    name: hiremebahamas-backend
    runtime: python
    region: oregon
    
    # ==========================================================================
    # PLAN SELECTION - PRODUCTION-IMMORTAL CONFIGURATION
    # ==========================================================================
    # 
    # STARTER PLAN ($7/month) - RECOMMENDED FOR LAUNCH:
    #   - ALWAYS ON - Never sleeps, no 502 errors ever
    #   - 512MB RAM - Optimized for Gunicorn 2 workers + 8 threads
    #   - Instant response times (<1 second) even after hours of inactivity
    #
    # STANDARD PLAN ($25/month) - RECOMMENDED FOR GROWTH:
    #   - Always On + Auto-scaling support (1-10 instances)
    #   - 2GB RAM - Supports 4 workers + 8 threads each
    #   - Best for high-traffic applications (>1000 concurrent users)
    #
    # Memory Budget Analysis (Starter 512MB):
    #   - Gunicorn master: ~50MB
    #   - Worker 1: ~150MB (with preload memory sharing)
    #   - Worker 2: ~150MB (with preload memory sharing)
    #   - Redis client: ~10MB
    #   - Connection pool: ~20MB
    #   - Overhead: ~50MB
    #   - Total: ~430MB < 512MB ✅
    # ==========================================================================
    plan: starter
    
    # Auto-scaling configuration (requires Standard plan $25/mo or higher)
    scaling:
      minInstances: 1        # Minimum instances (always running)
      maxInstances: 10       # Maximum instances during peak load
      targetCPUPercent: 70   # Scale up when average CPU > 70%
      targetMemoryPercent: 80 # Scale up when average memory > 80%
    
    buildCommand: |
      apt-get update
      apt-get install -y --no-install-recommends \
        build-essential \
        gcc \
        g++ \
        libpq-dev \
        libpq5 \
        python3-dev \
        libssl-dev \
        libffi-dev \
        ca-certificates
      pip install --upgrade pip setuptools wheel
      pip install -r requirements.txt
    # ==========================================================================
    # START COMMAND - Cold Start Elimination Configuration
    # ==========================================================================
    # 
    # Uses Gunicorn with --preload to eliminate 30-120s cold starts:
    #   --preload: Load app BEFORE forking workers (instant first request)
    #   --config: Use gunicorn.conf.py for worker/thread/timeout settings
    # 
    # PERFORMANCE TARGETS:
    #   Cold start: <12 seconds total
    #   First request: <400ms
    #   Login: <200ms (Redis-cached)
    #   Feed/Profile: <150ms
    #   RAM: <450MB
    # ==========================================================================
    startCommand: gunicorn final_backend_postgresql:application --config gunicorn.conf.py --preload
    envVars:
      - key: FLASK_ENV
        value: production
      - key: ENVIRONMENT
        value: production
      - key: SECRET_KEY
        generateValue: true
      - key: FRONTEND_URL
        value: https://hiremebahamas.vercel.app
      - key: PYTHONUNBUFFERED
        value: "true"
      - key: PYTHON_VERSION
        value: "3.12.0"
      # ==========================================================================
      # WORKER CONFIGURATION - OPTIMIZED FOR <450MB RAM
      # ==========================================================================
      # Memory analysis (Starter 512MB plan):
      #   Master: 50MB, Workers: 2x150MB, Pool: 30MB, Overhead: 50MB = ~430MB
      # ==========================================================================
      - key: WEB_CONCURRENCY
        value: "2"
      - key: WEB_THREADS
        value: "8"
      - key: PRELOAD_APP
        value: "true"
      - key: DB_KEEPALIVE_ENABLED
        value: "true"
      # ==========================================================================
      # LOAD BALANCER CONFIGURATION
      # ==========================================================================
      # FORWARDED_ALLOW_IPS: Accept X-Forwarded-* headers from Render's load balancer
      # Value "*" is safe on Render because:
      # - Render's infrastructure handles all external traffic
      # - Direct connections to containers are not possible from outside
      # - Load balancer strips/replaces X-Forwarded headers on ingress
      # For self-hosted deployments, restrict to specific IP ranges
      # ==========================================================================
      - key: FORWARDED_ALLOW_IPS
        value: "*"
      # ==========================================================================
      # DATABASE CONFIGURATION - RAILWAY POSTGRES OPTIMIZED
      # ==========================================================================
      # Set DATABASE_URL in Render dashboard with these parameters:
      # postgresql://user:pass@host:port/db?sslmode=require&connect_timeout=10&jit=off
      #
      # Critical settings:
      #   - jit=off: Disables JIT compilation (faster for simple queries)
      #   - sslmode=require: Required for Railway
      #   - connect_timeout=10: Fail fast on connection issues
      # ==========================================================================
      - key: STATEMENT_TIMEOUT_MS
        value: "30000"
      - key: DB_POOL_MAX_CONNECTIONS
        value: "20"
      - key: DB_POOL_MIN_CONNECTIONS
        value: "2"
      - key: DB_POOL_RECYCLE_SECONDS
        value: "180"
      # ==========================================================================
      # REDIS CACHE CONFIGURATION - UPSTASH OR RAILWAY REDIS
      # ==========================================================================
      # Set REDIS_URL in Render dashboard:
      #   Upstash: rediss://default:xxx@xxx.upstash.io:6379
      #   Railway: redis://default:xxx@xxx.railway.app:6379
      #
      # Cache TTLs (in seconds):
      #   Login: 600 (10 min) - Skip DB query on repeat login
      #   Profile: 60 (1 min) - User profile data
      #   Posts: 30 (30 sec) - Dynamic content
      #   Jobs: 60 (1 min) - Job listings
      # ==========================================================================
      - key: CACHE_TIMEOUT_LOGIN_USER
        value: "600"
      - key: CACHE_TIMEOUT_PROFILE
        value: "60"
      - key: CACHE_TIMEOUT_POSTS
        value: "30"
      - key: CACHE_TIMEOUT_JOBS
        value: "60"
      - key: CACHE_DEFAULT_TIMEOUT
        value: "300"
      # ==========================================================================
      # BCRYPT CONFIGURATION - OPTIMIZED FOR SPEED
      # ==========================================================================
      # 10 rounds = ~60ms per hash (vs 12 rounds = ~240ms)
      # OWASP recommends minimum 10 rounds for security
      # ==========================================================================
      - key: BCRYPT_ROUNDS
        value: "10"
      - key: PASSWORD_HASH_MIGRATION_ENABLED
        value: "true"
      # ==========================================================================
      # GUNICORN CONFIGURATION - PREVENT OOM AND TIMEOUTS
      # ==========================================================================
      - key: GUNICORN_TIMEOUT
        value: "55"
      - key: GUNICORN_GRACEFUL_TIMEOUT
        value: "30"
      - key: GUNICORN_KEEPALIVE
        value: "5"
      - key: GUNICORN_MAX_REQUESTS
        value: "500"
      - key: GUNICORN_MAX_REQUESTS_JITTER
        value: "50"
    
    # ==========================================================================
    # HEALTH CHECK CONFIGURATION - IMMORTAL SETTINGS
    # ==========================================================================
    # 
    # EXACT RENDER DASHBOARD SETTINGS (copy-paste these values):
    #   Health Check Path: /health
    #   Grace Period: 180 (seconds - gives cold boot time to complete)
    #   Timeout: 10 (seconds - per-request timeout, our endpoint responds in <10ms)
    #   Interval: 30 (seconds - how often Render checks health)
    #
    # These settings make health check failures PHYSICALLY IMPOSSIBLE because:
    # 1. /health responds instantly (<10ms) with no DB access
    # 2. Supports both GET and HEAD HTTP methods
    # 3. 180-second grace period covers any cold start
    # 4. 10-second timeout is 1000x longer than our response time
    #
    # Available endpoints:
    #   /health        - IMMORTAL health (instant, no DB, responds in <10ms, GET+HEAD)
    #   /ready         - Database readiness probe (checks Postgres, returns 503 if DB down)
    #   /ping          - Lightweight ping (fastest, no DB, for external pingers)
    #   /api/health    - Detailed health (includes DB status, for monitoring)
    #
    # External pinger URL: https://hiremebahamas.onrender.com/ping
    #
    # ONE-LINE FORCE REDEPLOY COMMAND (after changing healthcheck path):
    #   In Render Dashboard: Settings → Manual Deploy → Deploy latest commit
    #   Or: Click "Deploy" button in the service header
    # ==========================================================================
    healthCheckPath: /health

  # ==========================================================================
  # KEEP-ALIVE BACKGROUND WORKER (RECOMMENDED - More Frequent Pings)
  # ==========================================================================
  # 
  # This background worker pings the web service every 50 seconds with:
  # - Exponential backoff on failures
  # - Cache warming integration
  # - Structured logging
  # 
  # Benefits over cron:
  # - More frequent pings (50s vs 5min)
  # - Faster recovery from sleep
  # - Cache warming on each ping
  # - Better reliability with backoff
  # 
  # Note: If using Starter plan ($7/mo), this is optional but adds reliability.
  # ==========================================================================
  - type: worker
    name: keep-alive
    runtime: python
    region: oregon
    plan: free
    buildCommand: pip install requests
    startCommand: python keep_alive.py
    envVars:
      - key: PYTHONUNBUFFERED
        value: "true"

  # ==========================================================================
  # CACHE WARMING CRON JOB - Meta-style Hot Path Caching
  # ==========================================================================
  # 
  # This cron job pre-warms the cache every 5 minutes to ensure
  # sub-100ms response times for frequently accessed data.
  # 
  # Note: The background worker already does cache warming, but this
  # provides a fallback if the worker is down.
  # ==========================================================================
  - type: cron
    name: cache-warmer
    region: oregon
    plan: free
    schedule: "*/5 * * * *"
    dockerImage: curlimages/curl:latest
    dockerCommand: curl -f -s -X POST https://hiremebahamas.onrender.com/warm-cache
